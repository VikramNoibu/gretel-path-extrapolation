{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "danish-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from main import *\n",
    "import random, string\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from metrics import compute_topk_contains_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-making",
   "metadata": {},
   "source": [
    "## Setup Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "treated-lloyd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "domain = 'pollini'\n",
    "config_path = \"config/\" + domain\n",
    "config = Config()\n",
    "config.load_from_file(config_path)\n",
    "\n",
    "data = \"data/\"+domain\n",
    "pages = torch.load(data+\"/pages.pt\")\n",
    "actions = torch.load(data+\"/actions.pt\")\n",
    "node_ids= torch.load(data+\"/n2f.pt\")\n",
    "nodes2labels = {key: pages[page]+\"_\"+actions[action] for key, (page,action) in node_ids.items()}\n",
    "logging = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continuous-ceremony",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Cart_Click',\n",
       " 1: 'Product_scroll',\n",
       " 2: 'Cart_placeorder',\n",
       " 3: 'Product_addtocart',\n",
       " 4: 'ContactUs_scroll',\n",
       " 5: 'Home_checkout',\n",
       " 6: 'info_Click',\n",
       " 7: 'info_placeorder',\n",
       " 8: 'Category_Click',\n",
       " 9: 'Category_placeorder',\n",
       " 10: 'placeorder_scroll',\n",
       " 11: 'UserAccount_checkout',\n",
       " 12: 'Cart_checkout',\n",
       " 13: 'Product_Click',\n",
       " 14: 'Product_placeorder',\n",
       " 15: 'ContactUs_Click',\n",
       " 16: 'info_checkout',\n",
       " 17: 'Category_checkout',\n",
       " 18: 'Home_scroll',\n",
       " 19: 'placeorder_Click',\n",
       " 20: 'UserAccount_scroll',\n",
       " 21: 'placeorder_placeorder',\n",
       " 22: 'Product_checkout',\n",
       " 23: 'Cart_addtocart',\n",
       " 24: 'ContactUs_checkout',\n",
       " 25: 'Cart_scroll',\n",
       " 26: 'Home_Click',\n",
       " 27: 'info_scroll',\n",
       " 28: 'Category_addtocart',\n",
       " 29: 'info_addtocart',\n",
       " 30: 'Category_scroll',\n",
       " 31: 'placeorder_checkout',\n",
       " 32: 'UserAccount_Click'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes2labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-discipline",
   "metadata": {},
   "source": [
    "## Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unique-activation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_balance_acc(count, acc):\n",
    "    temp = count.vec_sum/acc.vec_sum\n",
    "    return torch.isfinite(temp).sum(), torch.isnan(temp).sum()\n",
    "\n",
    "def setup_model(config):\n",
    "    TRAJ_MAX_LEN=99\n",
    "    graph, trajectories, pairwise_node_features, _ = load_data(config)\n",
    "\n",
    "    model = create_model(graph, pairwise_node_features, config)\n",
    "    model = model.to(config.device)\n",
    "    train_trajectories, valid_trajectories, test_trajectories = trajectories\n",
    "    use_validation_set = len(valid_trajectories) > 0\n",
    "\n",
    "    graph = graph.to(config.device)\n",
    "    given_as_target, siblings_nodes = None, None\n",
    "\n",
    "    if config.enable_checkpointing:\n",
    "            chkpt_dir = os.path.join(\n",
    "                config.workspace, config.checkpoint_directory, config.name)\n",
    "            os.makedirs(chkpt_dir, exist_ok=True)\n",
    "\n",
    "    optimizer = create_optimizer(model.parameters(), config)\n",
    "\n",
    "    if config.restore_from_checkpoint:\n",
    "        filename = input(\"Checkpoint file: \")\n",
    "        checkpoint_data = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint_data[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_data[\"optimizer_state_dict\"])\n",
    "        print(\"Loaded parameters from checkpoint\")\n",
    "\n",
    "    if config.compute_baseline:\n",
    "        display_baseline(config, graph, train_trajectories,\n",
    "                         test_trajectories, create_evaluator())\n",
    "\n",
    "    graph = graph.add_self_loops(degree_zero_only=config.self_loop_deadend_only, edge_value=config.self_loop_weight)\n",
    "    return model, graph, pairwise_node_features, optimizer, train_trajectories, valid_trajectories, test_trajectories, given_as_target, siblings_nodes,use_validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "modern-starter",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_metrics(model1, model2, graph, n_obs1, n_obs2, trajectories, config, pairwise_features, alpha, k=5):\n",
    "    top1_max = []\n",
    "    top1_mean = []\n",
    "    m1 = []\n",
    "    m2 = []\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        for trajectory_idx in range(len(trajectories)):\n",
    "            observations = trajectories[trajectory_idx]\n",
    "\n",
    "            number_steps = None\n",
    "            if config.rw_edge_weight_see_number_step or config.rw_expected_steps:\n",
    "                if config.use_shortest_path_distance:\n",
    "                    number_steps = (\n",
    "                        trajectories.leg_shortest_lengths(\n",
    "                            trajectory_idx).float() * 1.1\n",
    "                    ).long()\n",
    "                else:\n",
    "                    number_steps = trajectories.leg_lengths(trajectory_idx)\n",
    "\n",
    "            ## Model 1\n",
    "            observed, starts, targets = generate_masks(\n",
    "                trajectory_length=observations.shape[0],\n",
    "                number_observations=n_obs1,\n",
    "                predict=config.target_prediction,\n",
    "                with_interpolation=config.with_interpolation,\n",
    "                device=config.device,\n",
    "            )\n",
    "\n",
    "            diffusion_graph = (\n",
    "                graph if not config.diffusion_self_loops else graph.add_self_loops()\n",
    "            )\n",
    "            \n",
    "            predictions1, _, rw_weights = model1(\n",
    "            observations,\n",
    "            graph,\n",
    "            diffusion_graph,\n",
    "            observed=observed,\n",
    "            starts=starts,\n",
    "            targets=targets,\n",
    "            pairwise_node_features=pairwise_features,\n",
    "            number_steps=number_steps)\n",
    "            len_targ1 = len(targets)\n",
    "            \n",
    "            ## Model 2\n",
    "            observed, starts, targets = generate_masks(\n",
    "                trajectory_length=observations.shape[0],\n",
    "                number_observations=n_obs2,\n",
    "                predict=config.target_prediction,\n",
    "                with_interpolation=config.with_interpolation,\n",
    "                device=config.device,\n",
    "            )\n",
    "            len_targ2 = len(targets)\n",
    "            predictions2, _, rw_weights = model2(\n",
    "            observations,\n",
    "            graph,\n",
    "            diffusion_graph,\n",
    "            observed=observed,\n",
    "            starts=starts,\n",
    "            targets=targets,\n",
    "            pairwise_node_features=pairwise_features,\n",
    "            number_steps=number_steps)\n",
    "\n",
    "            min_len = min(len_targ1, len_targ2)\n",
    "\n",
    "            ## Combine Models\n",
    "            target_distributions = observations[-min_len:, :]\n",
    "            predictions1 = predictions1[-min_len:,:]\n",
    "            predictions2 = predictions2[-min_len:,:]\n",
    "\n",
    "            #print(target_distributions.shape, predictions1.shape, predictions2.shape)\n",
    "            m1.append(compute_topk_contains_target(target_distributions, predictions1, k=k))\n",
    "            m2.append(compute_topk_contains_target(target_distributions, predictions2, k=k))\n",
    "\n",
    "            predictions = torch.max(torch.stack((predictions1, predictions2)),dim=0).values\n",
    "            top1_max.append(compute_topk_contains_target(target_distributions, predictions, k=k))\n",
    "\n",
    "            predictions = torch.add(predictions1*alpha, (1-alpha)*predictions2)\n",
    "            top1_mean.append(compute_topk_contains_target(target_distributions, predictions, k=k))\n",
    "            #print(predictions1.shape, predictions2.shape,compute_topk_contains_target(target_distributions, predictions, k=1) )\n",
    "    \n",
    "    top_max = torch.cat(top1_max)\n",
    "    top_mean = torch.cat(top1_mean)\n",
    "    m1_mean = torch.cat(m1)\n",
    "    m2_mean = torch.cat(m2)\n",
    "    \n",
    "    #print(top_max, top_mean)\n",
    "    return top_max.float().mean(), top_mean.float().mean(), m1_mean.float().mean(), m2_mean.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-kitty",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "filled-anaheim",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(config, logging):\n",
    "    model, graph, pairwise_node_features, optimizer, train_trajectories, valid_trajectories, test_trajectories, given_as_target, siblings_nodes,use_validation_set = setup_model(config)\n",
    "    for epoch in range(config.number_epoch):\n",
    "        print(\"EPOCH: \" , epoch)\n",
    "        model.train()\n",
    "\n",
    "        print_cum_loss = 0.0\n",
    "        print_num_preds = 0\n",
    "        print_time = time.time()\n",
    "        print_every = len(\n",
    "            train_trajectories) // config.batch_size // config.print_per_epoch\n",
    "\n",
    "        trajectories_shuffle_indices = np.arange(len(train_trajectories))\n",
    "        if config.shuffle_samples:\n",
    "            np.random.shuffle(trajectories_shuffle_indices)\n",
    "\n",
    "        for iteration, batch_start in enumerate(range(0, len(trajectories_shuffle_indices) - \n",
    "                                                      config.batch_size + 1, config.batch_size)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = torch.tensor(0.0, device=config.device)\n",
    "\n",
    "                for i in range(batch_start, batch_start + config.batch_size):\n",
    "                    trajectory_idx = trajectories_shuffle_indices[i]\n",
    "                    observations = train_trajectories[trajectory_idx]\n",
    "                    length = train_trajectories.lengths[trajectory_idx]\n",
    "\n",
    "                    number_steps = None\n",
    "                    if config.rw_edge_weight_see_number_step or config.rw_expected_steps:\n",
    "                        if config.use_shortest_path_distance:\n",
    "                            number_steps = (\n",
    "                                train_trajectories.leg_shortest_lengths(\n",
    "                                    trajectory_idx).float() * 1.1\n",
    "                            ).long()\n",
    "                        else:\n",
    "                            number_steps = train_trajectories.leg_lengths(\n",
    "                                trajectory_idx)\n",
    "\n",
    "                    observed, starts, targets = generate_masks(\n",
    "                        trajectory_length=observations.shape[0],\n",
    "                        number_observations=config.number_observations,\n",
    "                        predict=config.target_prediction,\n",
    "                        with_interpolation=config.with_interpolation,\n",
    "                        device=config.device,\n",
    "                    )\n",
    "\n",
    "                    diffusion_graph = graph if not config.diffusion_self_loops else graph.add_self_loops()\n",
    "\n",
    "                    predictions, potentials, rw_weights = model(\n",
    "                        observations,\n",
    "                        graph,\n",
    "                        diffusion_graph,\n",
    "                        observed=observed,\n",
    "                        starts=starts,\n",
    "                        targets=targets,\n",
    "                        pairwise_node_features=pairwise_node_features,\n",
    "                        number_steps=number_steps,\n",
    "                    )\n",
    "\n",
    "                    print_num_preds += starts.shape[0]\n",
    "\n",
    "                    l = (compute_loss(config.loss,train_trajectories, observations, predictions,starts,targets,rw_weights, trajectory_idx,)\n",
    "                        / starts.shape[0])\n",
    "                    loss += l\n",
    "\n",
    "                loss /= config.batch_size\n",
    "                print_cum_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (iteration + 1) % print_every == 0:\n",
    "                    print_loss = print_cum_loss / print_every\n",
    "                    print_loss /= print_num_preds\n",
    "                    pred_per_second = 1.0 * print_num_preds / \\\n",
    "                        (time.time() - print_time)\n",
    "\n",
    "                    print_cum_loss = 0.0\n",
    "                    print_num_preds = 0\n",
    "                    print_time = time.time()\n",
    "\n",
    "                    progress_percent = int(\n",
    "                        100.0 * ((iteration + 1) // print_every) /\n",
    "                        config.print_per_epoch\n",
    "                    )\n",
    "\n",
    "                    #print(\n",
    "                    #    f\"Progress {progress_percent}% | iter {iteration} | {pred_per_second:.1f} pred/s | loss {config.loss} {print_loss}\"\n",
    "                    #)\n",
    "        # VALID and TEST metrics computation\n",
    "        def create_evaluator():\n",
    "            return Evaluator(\n",
    "                graph.n_node,\n",
    "                given_as_target=given_as_target,\n",
    "                siblings_nodes=siblings_nodes,\n",
    "                config=config,\n",
    "            )\n",
    "        test_evaluator, node_acc_dist = evaluate(model,graph,test_trajectories,pairwise_node_features,create_evaluator,dataset=\"TEST\",)\n",
    "\n",
    "        to_fin, to_nan = get_balance_acc(node_acc_dist['node_to_count'],node_acc_dist['node_to_acc5'])\n",
    "        from_fin, from_nan = get_balance_acc(node_acc_dist['node_from_count'],node_acc_dist['node_from_acc5'])\n",
    "        zeros5_to = torch.count_nonzero(node_acc_dist['node_to_acc5'].vec_sum).sum()\n",
    "        zeros5_from = torch.count_nonzero(node_acc_dist[\"node_from_acc5\"].vec_sum).sum()\n",
    "        zeros1_to = torch.count_nonzero(node_acc_dist['node_to_acc1'].vec_sum).sum()\n",
    "        zeros1_from = torch.count_nonzero(node_acc_dist[\"node_from_acc1\"].vec_sum).sum()\n",
    "        \n",
    "        ## Checkpointing\n",
    "        if config.enable_checkpointing and epoch % config.chechpoint_every_num_epoch == 0:\n",
    "            # print(\"Checkpointing...\")\n",
    "            directory = os.path.join(\n",
    "                config.workspace, config.checkpoint_directory, config.name)\n",
    "            chkpt_file = os.path.join(directory, f\"{epoch:04d}.pt\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                chkpt_file,\n",
    "            )\n",
    "            config_file = os.path.join(directory, \"config\")\n",
    "            config.save_to_file(config_file)\n",
    "\n",
    "            metrics_file = os.path.join(directory, f\"{epoch:04d}.txt\")\n",
    "            with open(metrics_file, \"w\") as f:\n",
    "                f.write(test_evaluator.to_string())\n",
    "        ## log hyperparameters\n",
    "        logging.add_hparams({'batch_size': config.batch_size, 'lr': config.lr,\n",
    "                            'self_loop_weight': config.self_loop_weight,\n",
    "                            'loss':config.loss,\n",
    "                            'target': config.target_prediction, \"n_obs\":config.number_observations},\n",
    "                            {\"top1\":test_evaluator.metrics['precision_top1'].mean(), \n",
    "                            \"top5\":test_evaluator.metrics['precision_top5'].mean(),\n",
    "                            })\n",
    "    \n",
    "    accuracy = node_acc_dist['node_to_acc5'].vec_sum.cpu()/node_acc_dist['node_to_count'].vec_sum.cpu()\n",
    "    nodes_not_predicted = np.arange(len(accuracy))[accuracy<0.2]\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"Diversity:\", [nodes2labels[i] for i in nodes_not_predicted])\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    #return zeros1_to, zeros5_to, zeros1_from, zeros5_from, node_acc_dist, model\n",
    "    return model, graph, test_trajectories, pairwise_node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-texture",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "weekly-spray",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hyperparameters -- defaults\n",
    "logging = SummaryWriter()\n",
    "config.target_prediction = \"next\"\n",
    "config.lr = 0.002\n",
    "config.batch_size = 24\n",
    "config.self_loop_weight = 0.01\n",
    "config.loss = 'nll_loss'\n",
    "config.diffusion_k_hops = 1\n",
    "config.number_epoch = 3\n",
    "\n",
    "## Graph diffusion params\n",
    "rw_expected_steps = True\n",
    "rw_edge_weight_see_number_step = True\n",
    "with_interpolation = True\n",
    "initial_edge_transformer = True\n",
    "use_shortest_path_distance = False\n",
    "double_way_diffusion = False\n",
    "rw_non_backtracking = False\n",
    "diffusion_self_loops = True\n",
    "\n",
    "bools = [True, False]\n",
    "\n",
    "## Define grid\n",
    "lrs = [0.005]\n",
    "bss = [24]\n",
    "loop_weights = [0.01]\n",
    "loss = [\"nll_loss\"]\n",
    "logs = []\n",
    "#for num in num_obss:\n",
    "#    for hops in [2]:\n",
    "#        config.diffusion_k_hops = hops\n",
    "#        config.number_observations = num\n",
    "#        random_name = ''.join(random.choice(string.ascii_lowercase) for i in range(6))\n",
    "#        zeros1_to, zeros5_to, zeros1_from, zeros5_from, node_acc_dist = train(config, logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-country",
   "metadata": {},
   "source": [
    "## Combine two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "threaded-technique",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0\n",
      "\u001b[31m{'precision_top1': 0.43887255749610926, 'precision_top2': 0.635656233788691, 'precision_top3': 0.7065536918554384, 'precision_top5': 0.8077122600726266}\u001b[0m\n",
      "EPOCH:  1\n",
      "\u001b[31m{'precision_top1': 0.4625626837281688, 'precision_top2': 0.6764655023344285, 'precision_top3': 0.7499567698426423, 'precision_top5': 0.8689261628912329}\u001b[0m\n",
      "EPOCH:  2\n",
      "\u001b[31m{'precision_top1': 0.45737506484523605, 'precision_top2': 0.6714508040809268, 'precision_top3': 0.7527234999135397, 'precision_top5': 0.8734220992564413}\u001b[0m\n",
      "-----------------------------------------------------------\n",
      "Diversity: ['ContactUs_scroll', 'Product_placeorder', 'Category_checkout', 'Cart_scroll', 'Category_addtocart']\n",
      "-----------------------------------------------------------\n",
      "EPOCH:  0\n",
      "\u001b[31m{'precision_top1': 0.4564868336544637, 'precision_top2': 0.6204238921001927, 'precision_top3': 0.724791265253693, 'precision_top5': 0.8142260757867694}\u001b[0m\n",
      "EPOCH:  1\n",
      "\u001b[31m{'precision_top1': 0.4824983943481053, 'precision_top2': 0.6881824020552344, 'precision_top3': 0.7646114322414901, 'precision_top5': 0.8712267180475273}\u001b[0m\n",
      "EPOCH:  2\n",
      "\u001b[31m{'precision_top1': 0.4824983943481053, 'precision_top2': 0.6957289659601799, 'precision_top3': 0.7663776493256262, 'precision_top5': 0.8750802825947335}\u001b[0m\n",
      "-----------------------------------------------------------\n",
      "Diversity: ['ContactUs_scroll', 'Product_placeorder', 'Category_checkout', 'Category_addtocart']\n",
      "-----------------------------------------------------------\n",
      "0.8742867112159729 0.8682344555854797 0.8734220862388611 0.8671969175338745\n"
     ]
    }
   ],
   "source": [
    "n_obs1 = 2\n",
    "n_obs2 = 1\n",
    "config.number_observations = n_obs1\n",
    "model1, graph, test_trajectories, pairwise_node_features = train(config, logging)\n",
    "\n",
    "config.number_observations = n_obs2\n",
    "model2, graph, _, _ = train(config, logging)\n",
    "random_name = ''.join(random.choice(string.ascii_lowercase) for i in range(6))\n",
    "for alpha in [0.5]:\n",
    "    top_max, top_mean, m1, m2 = gen_metrics(model1, model2, graph, n_obs1, n_obs2, test_trajectories, config, pairwise_node_features, alpha)\n",
    "    print(top_max.item(), top_mean.item(), m1.item(), m2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "preliminary-liberal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]), tensor([9]), tensor([10]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_masks(11, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "polished-island",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [1, 2],\n",
       "         [2, 3],\n",
       "         [3, 4],\n",
       "         [4, 5],\n",
       "         [5, 6],\n",
       "         [6, 7],\n",
       "         [7, 8],\n",
       "         [8, 9]]),\n",
       " tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_masks(11, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-cloud",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-spectacular",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Prediction\")\n",
    "plt.bar(np.arange(node_acc_dist['node_to_acc5'].vec_sum.shape[0]), node_acc_dist['node_to_acc5'].vec_sum.cpu())\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Actual\")\n",
    "plt.bar(np.arange(node_acc_dist['node_to_acc5'].vec_sum.shape[0]), node_acc_dist['node_to_count'].vec_sum.cpu())\n",
    "#plt.ylim([0, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = node_acc_dist['node_to_acc5'].vec_sum.cpu()\n",
    "trues = node_acc_dist['node_to_count'].vec_sum.cpu()\n",
    "\n",
    "accuracy = preds/trues\n",
    "nodes_not_predicted = np.arange(len(accuracy))[accuracy<0.2]\n",
    "[nodes2labels[i] for i in nodes_not_predicted]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
